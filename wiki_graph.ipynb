{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "job queue size: 0\n",
      "Num Visited entities: 12\n",
      "Graph diameter: 2\n",
      "Minimum depth: 0\n",
      "Entity at minimum depth: Baden-WÃ¼rttemberg\n",
      "[('Albert_Einstein', 'academicAdvisors', 'Heinrich_Friedrich_Weber', 'Heinrich Friedrich Weber'), ('Heinrich_Friedrich_Weber', 'birthPlace', 'Saxe-Weimar-Eisenach', 'Saxe-Weimar-Eisenach')]\n",
      "Time taken: 7.429980278015137\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import queue\n",
    "import concurrent\n",
    "import threading\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs_shortest_path(graph, start, end):\n",
    "    visited = set()\n",
    "    queue = deque([(start, [start])])\n",
    "    while queue:\n",
    "        node, path = queue.popleft()\n",
    "        if node == end:\n",
    "            return len(path) - 1  # return the length of the path\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            for neighbor in graph.get(node, {}):  # use dict.get to handle missing keys\n",
    "                queue.append((neighbor, path + [neighbor]))\n",
    "\n",
    "def graph_diameter(graph):\n",
    "    max_diameter = 0\n",
    "    all_nodes = set(graph.keys())  # nodes with outgoing edges\n",
    "    for node in graph.values():  # add nodes with incoming edges\n",
    "        all_nodes.update(node.keys())\n",
    "    all_nodes_list = list(all_nodes)\n",
    "    all_nodes = set(random.sample(all_nodes_list, min(100, len(all_nodes_list))))\n",
    "    for node in all_nodes:\n",
    "        for target_node in all_nodes:\n",
    "            if node != target_node:\n",
    "                path_length = bfs_shortest_path(graph, node, target_node)\n",
    "                if path_length is not None:\n",
    "                    max_diameter = max(max_diameter, path_length)\n",
    "    return max_diameter\n",
    "\n",
    "REQUEST_TIMEOUT = 5\n",
    "MIN_DEPTH = 100\n",
    "entity_min_depth = ''\n",
    "max_threads = 20\n",
    "executor = ThreadPoolExecutor(max_workers=max_threads)\n",
    "job_queue = queue.Queue()\n",
    "futures = []  # Define the futures list to keep track of submitted tasks\n",
    "thread_lock = threading.Lock()\n",
    "visited = set()\n",
    "def exploreWikidata(entity, graph, depth=3, item_labels=None, property_labels=None):\n",
    "    global MIN_DEPTH, entity_min_depth, job_queue, futures, executor, visited\n",
    "    try:\n",
    "        if depth == 0 or visited is None or item_labels is None or property_labels is None:\n",
    "            MIN_DEPTH = depth\n",
    "            entity_min_depth = entity\n",
    "            return\n",
    "        time.sleep(1.0)\n",
    "        relations = queryWikidataForRelations(entity)\n",
    "        #print(f\"Exploring {entity} at depth {depth} and length of relations {len(relations)}\")\n",
    "        for relatedEntity, propertyRelation, itemLabel, propertyLabel in relations:\n",
    "            item_labels[relatedEntity] = itemLabel\n",
    "            property_labels[propertyRelation] = propertyLabel\n",
    "            \n",
    "            #print(f\"Entity {entity} ({item_labels.get(entity)}) has relation {property_labels.get(propertyRelation)} with {relatedEntity} ({itemLabel})\")\n",
    "            if entity not in graph:\n",
    "                graph[entity] = {}\n",
    "            graph[entity][relatedEntity] = propertyRelation\n",
    "            if depth < MIN_DEPTH:\n",
    "                MIN_DEPTH = depth\n",
    "                entity_min_depth = entity\n",
    "            if relatedEntity not in visited:\n",
    "                with thread_lock:\n",
    "                    visited.add(relatedEntity)\n",
    "                job_queue.put((relatedEntity, graph, depth - 1, item_labels, property_labels))\n",
    "            #print(f\"Job queue1 size: {job_queue.qsize()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        raise(e)\n",
    "\n",
    "def queryWikidataForRelations(entity):\n",
    "    # The updated SPARQL query based on your input\n",
    "    #print(entity)\n",
    "    sparql = f\"\"\"\n",
    "    SELECT ?relatedEntity ?propertyRelation ?relatedEntityLabel ?propertyLabel\n",
    "    WHERE {{\n",
    "        <http://dbpedia.org/resource/{entity}> ?propertyRelation ?relatedEntity .\n",
    "        ?relatedEntity rdfs:label ?relatedEntityLabel .\n",
    "        ?propertyRelation rdfs:label ?propertyLabel .\n",
    "        FILTER (lang(?relatedEntityLabel) = \"en\")\n",
    "        FILTER (lang(?propertyLabel) = \"en\")\n",
    "        FILTER (?propertyRelation != <http://dbpedia.org/ontology/wikiPageWikiLink>)\n",
    "    }}\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://dbpedia.org/sparql'\n",
    "    try:\n",
    "        data = requests.get(url, params={'query': sparql, 'format': 'json'}, timeout=REQUEST_TIMEOUT).json()\n",
    "        relatedEntities = [\n",
    "            (\n",
    "                binding['relatedEntity']['value'].split('/')[-1], \n",
    "                binding['propertyRelation']['value'].split('/')[-1],\n",
    "                binding['relatedEntityLabel']['value'],\n",
    "                binding['propertyLabel']['value']\n",
    "            )\n",
    "            for binding in data['results']['bindings']\n",
    "        ]\n",
    "        #time.sleep(1.0)  # To avoid hitting the server too frequently\n",
    "        return list(random.sample(relatedEntities, min(3, len(relatedEntities))))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Query for entity {entity} timed out.\" + str(e)[:200])\n",
    "        return []\n",
    "    \n",
    "def DFS(entity, graph, visited, current_path, all_paths, item_labels, property_labels):\n",
    "    visited.add(entity)\n",
    "    is_leaf = True\n",
    "\n",
    "    for next_entity, property_id in graph.get(entity, {}).items():\n",
    "        if next_entity not in visited:\n",
    "            is_leaf = False\n",
    "            current_relation = (entity, property_id, next_entity, item_labels[next_entity])\n",
    "            current_path.append(current_relation)\n",
    "            DFS(next_entity, graph, visited, current_path, all_paths, item_labels, property_labels)\n",
    "            current_path.pop()  # Backtrack\n",
    "\n",
    "    if is_leaf:\n",
    "        all_paths.append(list(current_path))\n",
    "\n",
    "\n",
    "num_depth = 2\n",
    "start_time = time.time()\n",
    "print()\n",
    "startEntity = \"Albert_Einstein\"  # Starting from the entity representing a cat\n",
    "visited = set([startEntity])  # Dictionary to store entity labels (for more readable printout)\n",
    "graph = {}\n",
    "item_labels = {startEntity: \"Albert Einstein\"}\n",
    "property_labels = {}\n",
    "\n",
    "job_queue.put((startEntity, graph, num_depth, item_labels, property_labels))\n",
    "\n",
    "while True:\n",
    "    if job_queue.empty() and all(f.done() for f in futures):\n",
    "            break  # Exit the loop if the job queue is empty and all futures are done\n",
    "    if not job_queue.empty() or [f for f in futures if not f.done()]:\n",
    "        incomplete_futures = [f for f in futures if not f.done()]\n",
    "        if len(incomplete_futures) < max_threads and not job_queue.empty():\n",
    "            args = job_queue.get()\n",
    "            futures.append(executor.submit(exploreWikidata, *args))\n",
    "print(f\"job queue size: {job_queue.qsize()}\")\n",
    "#print(f\"Number of futures: {len(futures)}\", futures)\n",
    "executor.shutdown(wait=True)\n",
    "#print(f\"Graph: {graph}\")\n",
    "print(f\"Num Visited entities: {len(visited)}\")\n",
    "print(f\"Graph diameter: {graph_diameter(graph)}\")\n",
    "#print(f\"Item labels: {item_labels}\")\n",
    "#print(f\"Property labels: {property_labels}\")\n",
    "print(f\"Minimum depth: {MIN_DEPTH}\")\n",
    "print(f\"Entity at minimum depth: {entity_min_depth}\")\n",
    "\n",
    "path = []\n",
    "vis = set()\n",
    "all_paths = []\n",
    "DFS(startEntity, graph, vis, path, all_paths, item_labels, property_labels)\n",
    "#print(\"All paths: \")\n",
    "#print(all_paths)\n",
    "maxi = max(all_paths, key=len)\n",
    "print(maxi)\n",
    "end_tiem = time.time()\n",
    "print(f\"Time taken: {end_tiem - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "Fetching https://en.wikipedia.org/wiki/Heinrich_Friedrich_Weber\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def dbpedia_id_to_wikipedia_url(dbpedia_id):\n",
    "    \"\"\"Convert a DBpedia ID to its corresponding Wikipedia URL.\"\"\"\n",
    "    return \"https://en.wikipedia.org/wiki/\"+dbpedia_id\n",
    "\n",
    "def fetch_wikipedia_content(wikipedia_url):\n",
    "    \"\"\"Fetch the content of a Wikipedia page given its URL.\"\"\"\n",
    "    print(f\"Fetching {wikipedia_url}\")\n",
    "    response = requests.get(wikipedia_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: Could not fetch {wikipedia_url}\")\n",
    "        return None\n",
    "\n",
    "def save_to_txt(content, filename):\n",
    "    \"\"\"Save a string content to a .txt file.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "dbpedia_ids = [node[0] for node in maxi]\n",
    "\n",
    "for dbpedia_id in dbpedia_ids:\n",
    "    wikipedia_url = dbpedia_id_to_wikipedia_url(dbpedia_id)\n",
    "    content = fetch_wikipedia_content(wikipedia_url)\n",
    "    \n",
    "    if content:\n",
    "        # Use the last part of the DBpedia ID as the filename (e.g., \"Python_(programming_language).txt\")\n",
    "        filename = os.path.join(\"docs\", dbpedia_id.split(\"/\")[-1] + \".txt\")\n",
    "        save_to_txt(content, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
