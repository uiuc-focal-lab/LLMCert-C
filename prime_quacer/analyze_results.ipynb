{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import copy\n",
    "import shutil\n",
    "import json\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = 'results'\n",
    "model2dir = {}\n",
    "for folder in os.listdir(RESULTS_DIR):\n",
    "    model2dir[folder] = os.path.join(RESULTS_DIR, folder)\n",
    "    print(f\"{folder} has {len(os.listdir(f'{RESULTS_DIR}/{folder}'))} certificates\")\n",
    "len(model2dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CERTIFICATES = 10\n",
    "remove_models = []\n",
    "for model, dir in model2dir.items():\n",
    "    if len(os.listdir(dir)) < MIN_CERTIFICATES:\n",
    "        print(model)\n",
    "        remove_models.append(model)\n",
    "\n",
    "for model in remove_models:\n",
    "    del model2dir[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Certifications\n",
    "\n",
    "model2certificates = {}\n",
    "NUM_SAMPLES = 250\n",
    "for model, dir in model2dir.items():\n",
    "    model2certificates[model] = {}\n",
    "    for cert in os.listdir(dir):\n",
    "        if not cert.endswith('.pkl'):\n",
    "            continue\n",
    "        cert_path = os.path.join(dir, cert)\n",
    "        experiment_results = pickle.load(open(cert_path, 'rb'))\n",
    "        if len(experiment_results) == 3: #if we store (detailed_results, correct answers num , total queries num)\n",
    "            experiment_results = experiment_results[0]\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, result in enumerate(experiment_results):\n",
    "            if i >= NUM_SAMPLES:\n",
    "                break\n",
    "            correct += result['result'][0]\n",
    "            total += 1\n",
    "        (lower, upper) = proportion_confint(correct, total, alpha=0.05, method='beta')\n",
    "        model2certificates[model][cert] = (correct, total, lower, upper, correct/total)\n",
    "\n",
    "#Print summary\n",
    "\n",
    "model_result_summary = {}\n",
    "for model, certificates in model2certificates.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    certificates_lower = [cert[2] for cert in certificates.values()]\n",
    "    certificates_upper = [cert[3] for cert in certificates.values()]\n",
    "    certificates_acc = [cert[4] for cert in certificates.values()]\n",
    "    # print(certificates_acc)\n",
    "    print(f\" Mean lower bound: {np.mean(certificates_lower)}, Mean upper bound: {np.mean(certificates_upper)}, Mean accuracy: {np.mean(certificates_acc)}\")\n",
    "    print(f\"Std lower bound: {np.std(certificates_lower)}, Std upper bound: {np.std(certificates_upper)}, Sd accuracy: {np.std(certificates_acc)}\")\n",
    "    # print(f\"Min lower bound: {np.min(certificates_lower)}, Min upper bound: {np.min(certificates_upper)}, Min accuracy: {np.min(certificates_acc)}\")\n",
    "    # print(f\"Max lower bound: {np.max(certificates_lower)}, Max upper bound: {np.max(certificates_upper)}, Max accuracy: {np.max(certificates_acc)}\")\n",
    "    model_result_summary[model] = (np.mean(certificates_lower), np.mean(certificates_upper), np.mean(certificates_acc), np.std(certificates_lower), np.std(certificates_upper), np.std(certificates_acc))\n",
    "\n",
    "#Save summary\n",
    "SAVE_FILE = 'model_summary.json'\n",
    "json.dump(model_result_summary, open(SAVE_FILE, 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group results by base model\n",
    "model_groups = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for model_name, stats in model_result_summary.items():\n",
    "    # Parse model name components\n",
    "    parts = model_name.split('_')\n",
    "    \n",
    "    # Extract model family and size\n",
    "    if 'phi' in model_name:\n",
    "        model_family = 'Phi-3'\n",
    "        size = parts[1].upper().replace(\"B\", \"B)\")\n",
    "    elif 'llama' in model_name:\n",
    "        model_family = 'Llama'\n",
    "        size = parts[1].upper().replace(\"B\", \"B)\")\n",
    "    elif 'mistral' in model_name:\n",
    "        model_family = 'Mistral'\n",
    "        size = parts[1].upper().replace(\"B\", \"B)\")\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Extract precision and specification\n",
    "    if '4bit' in model_name:\n",
    "        precision = '4bit'\n",
    "        spec = parts[-1]\n",
    "    else:  # fp16 case\n",
    "        precision = 'fp16' \n",
    "        spec = parts[-1]\n",
    "    \n",
    "    # Format specification name\n",
    "    spec = spec.replace('_', ' ').title()\n",
    "    print(spec)\n",
    "    \n",
    "    # Store results with model grouping key\n",
    "    key = f\"{model_family} ({size}\"\n",
    "    model_groups[key][precision].append({\n",
    "        'spec': spec,\n",
    "        'lower': f\"{np.round(stats[0], 2):.2f} \\pm {np.round(stats[3], 2):.2f}\",\n",
    "        'upper': f\"{np.round(stats[1], 2):.2f} \\pm {np.round(stats[4], 2):.2f}\",\n",
    "        'acc': f\"{np.round(stats[2], 2):.2f} \\pm {np.round(stats[5], 2):.2f}\"\n",
    "    })\n",
    "\n",
    "# Generate LaTeX table rows\n",
    "latex_rows = []\n",
    "spec_order = ['Vanilla', 'Distractor']\n",
    "\n",
    "for model, precisions in model_groups.items():\n",
    "    num_rows = sum(len(specs) for specs in precisions.values())\n",
    "    latex_rows.append(rf\"\\multirow{{{num_rows}}}{{*}}{{\\makecell{{{model}}}}}\")\n",
    "    \n",
    "    for precision, specs in precisions.items():\n",
    "        sorted_specs = sorted(specs, key=lambda x: spec_order.index(x['spec']))\n",
    "        \n",
    "        for i, spec in enumerate(sorted_specs):\n",
    "            baseline = ''\n",
    "            if spec['spec'] == 'Shuffle':\n",
    "                baseline = '$todo \\pm todo$'  # Update baseline values from your data\n",
    "                \n",
    "            row = [\n",
    "                rf\"& \\multirow{{3}}{{*}}{{{precision}}}\" if i == 0 else \"&\",\n",
    "                f\"& {baseline}\" if baseline else \"& -\",\n",
    "                f\"& {spec['spec']}\",\n",
    "                f\"& ${spec['lower']}$\",\n",
    "                f\"& ${spec['upper']}$\",\n",
    "                f\"& ${spec['acc']}$ \\\\\\\\\"\n",
    "            ]\n",
    "            latex_rows.append(' '.join(row))\n",
    "        \n",
    "        if precision != list(precisions.keys())[-1]:\n",
    "            latex_rows.append(r\"\\cline{2-7}\\noalign{\\smallskip}\")\n",
    "\n",
    "    latex_rows.append(r\"\\midrule\")\n",
    "\n",
    "# Final table assembly\n",
    "latex_table = r\"\"\"\n",
    "\\begin{table*}[!tb]\n",
    "\\caption{Certification Results for Different LLMs}\n",
    "\\vspace{0.1cm}\n",
    "\\centering\n",
    "\\scriptsize{\n",
    "\\begin{tabular}{@{} p{1cm} ccclrrr@{}}\n",
    "\\toprule\n",
    "Model & Precision & Baseline & Specification Kind & \\makecell{Avg. \\\\ Lower Bound} & \\makecell{Avg. \\\\ Upper Bound} & \\makecell{Avg. \\\\ Accuracy} \\\\\n",
    "\\midrule\n",
    "\"\"\" + \"\\n\".join(latex_rows) + r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}}\n",
    "\\label{tab:certificates}\n",
    "\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We may need this for more detailed analysis\n",
    "from utils import load_aliases\n",
    "prime_folder = 'quacer_c_prime'\n",
    "actual_rels = json.load(open(os.path.join(prime_folder, 'actual_rels.json')))\n",
    "edge2src = json.load(open(os.path.join(prime_folder, 'edge2src.json')))\n",
    "graph = json.load(open(os.path.join(prime_folder, 'graph.json')))\n",
    "id2name = json.load(open(os.path.join(prime_folder, 'id2name.json')))\n",
    "id2source = json.load(open(os.path.join(prime_folder, 'id2source.json')))\n",
    "rels = json.load(open(os.path.join(prime_folder, 'rels.json')))\n",
    "graph_text_edge = json.load(open(os.path.join(prime_folder, 'graph_text_edge.json')))\n",
    "entity_aliases = load_aliases(os.path.join(prime_folder, 'entity_aliases.txt'))\n",
    "relations_aliases = load_aliases(os.path.join(prime_folder, 'relation_aliases.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {k:[] for k in model2dir.keys()}\n",
    "results = {k:[] for k in model2dir.keys()}\n",
    "results_data = {k:[] for k in model2dir.keys()}\n",
    "all_entities = []\n",
    "results_ids = {k:{} for k in model2dir.keys()}\n",
    "question_answers_models = {}\n",
    "model2ks = {}\n",
    "NUM_SAMPLES = 250\n",
    "for model, dir in model2dir.items():\n",
    "    ks = set()\n",
    "    for file in os.listdir(dir):\n",
    "        try:\n",
    "            experiment_results = pickle.load(open(os.path.join(dir, file), 'rb'))\n",
    "        except:\n",
    "            print(os.path.join(dir, file))\n",
    "            continue\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        idx = file.index('.')\n",
    "        all_entities.append(file[:idx+1])\n",
    "        model_files[model].append(file[:idx+1])\n",
    "        unique_queries = {}\n",
    "        repeat_queries = 0\n",
    "        unique_paths = set()\n",
    "        if len(experiment_results) < 10:\n",
    "            experiment_results = experiment_results[0]\n",
    "        for i, result in enumerate(experiment_results):\n",
    "            if i >= NUM_SAMPLES:\n",
    "                break\n",
    "            if type(result) == str:\n",
    "                print(result)\n",
    "                print(os.path.join(dir, file))\n",
    "                raise ValueError('Error in file: ', file)\n",
    "            if result['result'][0] == 1:\n",
    "                correct += 1\n",
    "            if result['question'] not in unique_queries:\n",
    "                unique_queries[result['question']] = []\n",
    "            path = tuple(result['path_id'])\n",
    "            ks.add(len(path)-1)\n",
    "            unique_paths.add(path)\n",
    "            certi = file[:idx+1]\n",
    "            if certi not in question_answers_models:\n",
    "                question_answers_models[certi]= {}\n",
    "            \n",
    "            if model not in question_answers_models[certi]:\n",
    "                question_answers_models[certi][model] = []\n",
    "            if 'distractor' in result:\n",
    "                question_answers_models[certi][model].append({'query': result['question'], 'eval': result['result'][0], \n",
    "                                                         'context': result['context'], 'model_answer':result['model_answer'], \n",
    "                                                         'correct_answers':result['correct_answers'], 'correct_ids':result['correct_ids'],\n",
    "                                                         'answer_options':result['options'], 'correct_ans_num':result['correct_ans_num'], \n",
    "                                                         'distractor':result['distractor'], 'path_id':result['path_id']})\n",
    "            else:\n",
    "                question_answers_models[certi][model].append({'query': result['question'], 'eval': result['result'][0], \n",
    "                                                         'context': result['context'], 'model_answer':result['model_answer'], \n",
    "                                                         'correct_answers':result['correct_answers'], 'correct_ids':result['correct_ids'],\n",
    "                                                         'answer_options':result['options'], 'correct_ans_num':result['correct_ans_num'], 'path_id':result['path_id']})\n",
    "            unique_queries[result['question']].append(result['result'][0])\n",
    "            total += 1\n",
    "        if total != NUM_SAMPLES:\n",
    "            raise ValueError('Error in file total: ', file, total)\n",
    "        results_ids[model][file[:idx+1]] = (correct, total, correct/total)\n",
    "        results[model].append(proportion_confint(correct, total, method='beta'))\n",
    "        repeat_queries = total - len(unique_queries)\n",
    "        same_query_accuracy = 0\n",
    "        count_same = 0\n",
    "        for query in unique_queries:\n",
    "            if len(unique_queries[query]) > 1:\n",
    "                if np.mean(unique_queries[query]) > 0:\n",
    "                    same_query_accuracy += np.mean(unique_queries[query])\n",
    "                    count_same += 1\n",
    "        if count_same == 0:\n",
    "            same_query_accuracy = 1.0\n",
    "        else:\n",
    "            same_query_accuracy = same_query_accuracy/count_same\n",
    "        results_data[model].append((correct, total, len(unique_queries), repeat_queries, same_query_accuracy))\n",
    "    model2ks[model] = ks\n",
    "print(\"total number of subgraphs: \", {key:len(value)for key, value in results.items()}, \n",
    "      \"\\nmean lower bounds per subgraph: \", {key:(np.mean([v[0] for v in value]), np.std([v[0] for v in value])) for key, value in results.items()}, \n",
    "      \"\\nmean upper bounds per subgraph: \", {key:(np.mean([v[1] for v in value]), np.std([v[1] for v in value])) for key, value in results.items()},\n",
    "      '\\nmean total questions per subgraph: ', {key:(np.mean([v[1] for v in value]), np.std([v[1] for v in value])) for key, value in results_data.items()}, \n",
    "      \"\\nmean correct answers per subgraph: \", {key:(np.mean([v[0] for v in results_data[key]]), np.std([v[1] for v in results_data[key]])) for key in results_data.keys()},\n",
    "      \"\\nmean accuracy per subgraph: \", {key:(np.mean([v[0]/v[1] for v in value]), np.std([v[0]/v[1] for v in value])) for key, value in results_data.items()},\n",
    "      \"\\nmean unique queries per subgraph: \", {key:(np.mean([v[2] for v in value]), np.std([v[2] for v in value])) for key, value in results_data.items()},\n",
    "      \"\\nmean repeat queries per subgraph: \", {key:(np.mean([v[3] for v in value]), np.std([v[3] for v in value])) for key, value in results_data.items()},\n",
    "      \"\\nmean same query accuracy per subgraph: \", {key:(np.mean([v[4] for v in value]), np.std([v[4] for v in value])) for key, value in results_data.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assuming 'question_answers_models' is defined and contains your data\n",
    "rows = []\n",
    "model_columns = []\n",
    "for certi, models in question_answers_models.items():\n",
    "    # Iterate over each model and their entries\n",
    "    for model_id, entries in models.items():\n",
    "        # Process each entry for the current model\n",
    "        for entry in entries:\n",
    "            path_id = entry['path_id']\n",
    "            path_names = [id2name[x] for x in path_id]\n",
    "            row = {'path_id': path_id, 'path_names':path_names, 'certi':certi}\n",
    "            # Include query, context, and correct answers\n",
    "            sampled_values = {'query': entry['query'], 'correct_answers': entry['correct_answers'], 'answer_options':entry['answer_options'], \n",
    "                            'correct_ans_num':entry['correct_ans_num'], 'context': entry['context']}\n",
    "            row.update(sampled_values)\n",
    "\n",
    "            # Include model's answer and its evaluation\n",
    "            model_answer_col = 'model_answer'\n",
    "            model_eval_col = 'model_eval'\n",
    "            row['model_id'] = model_id\n",
    "            row['correct_ids'] = entry['correct_ids']\n",
    "            row[model_answer_col] = entry['model_answer']\n",
    "            row[model_eval_col] = entry['eval']\n",
    "            row['answer_options'] = [id2name[opt] for opt in entry['answer_options']]\n",
    "            row['correct_ans_num'] = entry['correct_ans_num']\n",
    "            row['context'] = entry['context']\n",
    "            if 'distractor' in entry and entry['distractor'] is not None:\n",
    "                row['distractor'] = id2name[entry['distractor']]\n",
    "            else:\n",
    "                row['distractor'] = None\n",
    "\n",
    "            if model_answer_col not in model_columns:\n",
    "                model_columns.append(model_answer_col)\n",
    "            if model_eval_col not in model_columns:\n",
    "                model_columns.append(model_eval_col)\n",
    "            if 'model_id' not in model_columns:\n",
    "                model_columns.append('model_id')\n",
    "            if 'correct_ids' not in model_columns:\n",
    "                model_columns.append('correct_ids')\n",
    "            if 'answer_options' not in model_columns:\n",
    "                model_columns.append('answer_options')\n",
    "            if 'correct_ans_num' not in model_columns:\n",
    "                model_columns.append('correct_ans_num')\n",
    "            if 'distractor' not in model_columns:\n",
    "                model_columns.append('distractor')\n",
    "            if 'context' not in model_columns:\n",
    "                model_columns.append('context')\n",
    "\n",
    "            # Add the filled row to the rows list\n",
    "            rows.append(row)\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "static_columns = ['certi', 'path_id', 'query', 'correct_answers', 'path_names']\n",
    "ordered_columns = static_columns + sorted(model_columns)  # Sort or maintain order of model columns as needed\n",
    "df = df[ordered_columns]\n",
    "print(df.shape)\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "correct_df = df[df['model_eval'] == 1]\n",
    "incorrect_df = df[df['model_eval'] == 0]\n",
    "\n",
    "# Calculate the maximum index to ensure we don't go out of bounds\n",
    "max_index = len(df) - (len(df) % 10)\n",
    "\n",
    "# Generate a list of start indices\n",
    "start_indices = np.arange(0, max_index, 10)\n",
    "print(max_index)\n",
    "# List to hold the groups\n",
    "grouped_rows = []\n",
    "\n",
    "# Loop through each start index and get the consecutive 5 rows\n",
    "for start in start_indices:\n",
    "    if random.random() < 0.6:\n",
    "        continue\n",
    "    group = correct_df.iloc[start:start +1]  # Select the rows from 'start' to 'start+4'\n",
    "    grouped_rows.append(group)\n",
    "\n",
    "# Concatenate all the groups into a new DataFrame\n",
    "sampled_df = pd.concat(grouped_rows)\n",
    "#shuffle the rows\n",
    "sampled_df = sampled_df.sample(frac=1).reset_index(drop=True)\n",
    "print(sampled_df.shape)\n",
    "\n",
    "incorrect_sampled_rows = []\n",
    "start_indices = np.arange(0, len(incorrect_df), 2)\n",
    "for start in start_indices:\n",
    "    if random.random() < 0.1:\n",
    "        continue\n",
    "    group = incorrect_df.iloc[start:start + 1]\n",
    "    incorrect_sampled_rows.append(group)\n",
    "incorrect_sampled_df = pd.concat(incorrect_sampled_rows)\n",
    "incorrect_sampled_df = incorrect_sampled_df.sample(frac=1).reset_index(drop=True)\n",
    "print(incorrect_sampled_df.shape)\n",
    "# Print the resulting DataFrame\n",
    "sampled_df.head(15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quacer-b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
