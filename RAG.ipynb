{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-sequence-nq\", index_name=\"custom\", passages_path=\"/share/llm-verify/my_knowledge_dataset/my_knowledge_dataset\", index_path=\"/share/llm-verify/my_knowledge_dataset/my_knowledge_dataset_hnsw_index.faiss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vvjain3/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:179: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RagSequenceForGeneration(\n",
       "  (rag): RagModel(\n",
       "    (question_encoder): DPRQuestionEncoder(\n",
       "      (question_encoder): DPREncoder(\n",
       "        (bert_model): BertModel(\n",
       "          (embeddings): BertEmbeddings(\n",
       "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "            (position_embeddings): Embedding(512, 768)\n",
       "            (token_type_embeddings): Embedding(2, 768)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (encoder): BertEncoder(\n",
       "            (layer): ModuleList(\n",
       "              (0-11): 12 x BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (generator): BartForConditionalGeneration(\n",
       "      (model): BartModel(\n",
       "        (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "        (encoder): BartEncoder(\n",
       "          (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x BartEncoderLayer(\n",
       "              (self_attn): BartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): BartDecoder(\n",
       "          (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x BartDecoderLayer(\n",
       "              (self_attn): BartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): BartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize with RagRetriever to do everything in one forward call\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: [\" the hitchhiker's guide to the galaxy\"]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"What is Douglas Adams the author of?\", return_tensors=\"pt\")\n",
    "# targets = tokenizer(text_target=\"In Paris, there are 10 million people.\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].cuda()\n",
    "# labels = targets[\"input_ids\"].cuda()\n",
    "# outputs = model(input_ids=input_ids, labels=labels)\n",
    "outputs = model.generate(input_ids, num_beams=4, early_stopping=True)\n",
    "print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.weight', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# or use retriever separately\n",
    "model1 = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", use_dummy_dataset=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: [\" the hitchhiker's guide to the galaxy\"]\n"
     ]
    }
   ],
   "source": [
    "# 1. Encode\n",
    "question_hidden_states = model1.question_encoder(input_ids)[0]\n",
    "# 2. Retrieve\n",
    "docs_dict = retriever(input_ids.cpu().detach().numpy(), question_hidden_states.cpu().detach().numpy(), return_tensors=\"pt\")\n",
    "doc_scores = torch.bmm(\n",
    "    question_hidden_states.unsqueeze(1), docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2).cuda()\n",
    ").squeeze(1)\n",
    "# 3. Forward to generator\n",
    "outputs = model1.generate(\n",
    "    context_input_ids=docs_dict[\"context_input_ids\"].cuda(),\n",
    "    context_attention_mask=docs_dict[\"context_attention_mask\"].cuda(),\n",
    "    doc_scores=doc_scores.cuda()\n",
    ")\n",
    "print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [\" Q42 / can be seen on display at The Centre for Computing History in Cambridge.In 2018, John Lloyd presented an hour-long episode of the BBC Radio Four documentary Archive on 4, discussing Adams' private papers, which are held at St John's College, Cambridge. The episode is available online.Travessa Douglas Adams, a street at 27°35′21.8″S 48°39′44.0″W in São José, Santa Catarina, Brazil is named in Adams's honour.In March 2021 Unbound announced a crowdfunder for 42: the wildly improbable ideas of Douglas Adams, on the 20th anniversary of his death, a book based on Adams's papers, edited by Kevin Jon Davies.The annual Douglas Adams // what is douglas adams the author of?\", ' Q42 / Douglas Noël Adams (11 March 1952 – 11 May 2001) was an English author, humorist, and screenwriter, best known for The Hitchhiker\\'s Guide to the Galaxy. Originally a 1978 BBC radio comedy, The Hitchhiker\\'s Guide to the Galaxy developed into a \"trilogy\" of five books that sold more than 15\\xa0million copies in his lifetime. It was further developed into a television series, several stage plays, comics, a video game, and a 2005 feature film. Adams\\'s contribution to UK radio is commemorated in The Radio Academy\\'s Hall of Fame.Adams also wrote Dirk Gently\\'s Holistic Detective Agency (1987) and The // what is douglas adams the author of?', \" Q25169 / The Hitchhiker's Guide to the Galaxy is a comedy science fiction franchise created by Douglas Adams. Originally a 1978 radio comedy broadcast on BBC Radio 4, it was later adapted to other formats, including novels, stage shows, comic books, a 1981 TV series, a 1984 text adventure game, and 2005 feature film. The Hitchhiker's Guide to the Galaxy is an international multimedia phenomenon; the novels are the most widely distributed, having been translated into more than 30 languages by 2005. The first novel, The Hitchhiker's Guide to the Galaxy (1979), has been ranked fourth on the BBC's The // what is douglas adams the author of?\", ' Q25169 / Safe\", first appeared in 1986, in The Utterly Utterly Merry Comic Relief Christmas Book, a special large-print compilation of different stories and pictures that raised money for the then-new Comic Relief charity in the UK. The story also appears in some of the omnibus editions of the trilogy, and in The Salmon of Doubt. There are two versions of this story, one of which is slightly more explicit in its political commentary. A novel, Douglas Adams\\' Starship Titanic: A Novel, written by Terry Jones, is based on Adams\\'s computer game of the same name, Douglas Adams\\'s Starship Titanic, which in // what is douglas adams the author of?', ' Q25169 / his \"favorite philosopher is Douglas Adams\" and his favourite spaceship ever is in The Hitchhiker\\'s Guide. Musk said the attitude that Adams presented through The Hitchhiker\\'s Guide had influenced the vision behind both SpaceX and Tesla Motors. When Musk launched his Tesla Roadster into an elliptical heliocentric orbit as part of the initial test launch of the Falcon Heavy, he had a copy of Douglas Adams\\' novel The Hitchhiker\\'s Guide to the Galaxy in the glovebox, along with references to the book in the form of a towel and a sign on the dashboard that reads \"DON\\'T PANIC!\", as a // what is douglas adams the author of?']\n"
     ]
    }
   ],
   "source": [
    "print(\"Context:\", tokenizer.batch_decode(docs_dict[\"context_input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
