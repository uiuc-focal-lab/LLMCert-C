{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import copy\n",
    "import shutil\n",
    "import json\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = 'final_results'\n",
    "model2dir = {}\n",
    "for folder in os.listdir(RESULTS_DIR):\n",
    "    model2dir[folder] = os.path.join(RESULTS_DIR, folder)\n",
    "    print(f\"{folder} has {len(os.listdir(f'{RESULTS_DIR}/{folder}'))} certificates\")\n",
    "len(model2dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CERTIFICATES = 50\n",
    "remove_models = []\n",
    "for model, dir in model2dir.items():\n",
    "    if len(os.listdir(dir)) < MIN_CERTIFICATES:\n",
    "        print(model)\n",
    "        remove_models.append(model)\n",
    "\n",
    "for model in remove_models:\n",
    "    del model2dir[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Certifications\n",
    "\n",
    "model2certificates = {}\n",
    "NUM_SAMPLES = 250\n",
    "for model, dir in model2dir.items():\n",
    "    model2certificates[model] = {}\n",
    "    for cert in os.listdir(dir):\n",
    "        if not cert.endswith('.pkl'):\n",
    "            continue\n",
    "        cert_path = os.path.join(dir, cert)\n",
    "        experiment_results = pickle.load(open(cert_path, 'rb'))\n",
    "        if len(experiment_results) == 3: #if we store (detailed_results, correct answers num , total queries num)\n",
    "            experiment_results = experiment_results[0]\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, result in enumerate(experiment_results):\n",
    "            if i >= NUM_SAMPLES:\n",
    "                break\n",
    "            correct += result['result'][0]\n",
    "            total += 1\n",
    "        (lower, upper) = proportion_confint(correct, total, alpha=0.05, method='beta')\n",
    "        model2certificates[model][cert] = (correct, total, lower, upper, correct/total)\n",
    "\n",
    "#Print summary\n",
    "\n",
    "model_result_summary = {}\n",
    "for model, certificates in model2certificates.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    certificates_lower = [cert[2] for cert in certificates.values()]\n",
    "    certificates_upper = [cert[3] for cert in certificates.values()]\n",
    "    certificates_acc = [cert[4] for cert in certificates.values()]\n",
    "    print(f\" Mean lower bound: {np.mean(certificates_lower)}, Mean upper bound: {np.mean(certificates_upper)}, Mean accuracy: {np.mean(certificates_acc)}\")\n",
    "    print(f\"Std lower bound: {np.std(certificates_lower)}, Std upper bound: {np.std(certificates_upper)}, Sd accuracy: {np.std(certificates_acc)}\")\n",
    "    \n",
    "    model_result_summary[model] = (np.mean(certificates_lower), np.mean(certificates_upper), np.mean(certificates_acc), np.std(certificates_lower), np.std(certificates_upper), np.std(certificates_acc))\n",
    "\n",
    "#Save summary\n",
    "SAVE_FILE = 'model_summary.json'\n",
    "json.dump(model_result_summary, open(SAVE_FILE, 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below to get structures for more detailed analysis as outlined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We may need this for more detailed analysis\n",
    "#qa_graph = json.load(open('wikidata_graphs/wikidata_util.json'))\n",
    "# context_graph_edge = json.load(open('wikidata_graphs/wikidata_text_edge.json'))\n",
    "# graph_text_sentencized = json.load(open('wikidata_graphs/wikidata_sentencized.json'))\n",
    "#id2name = json.load(open('wikidata_graphs/wikidata_name_id.json'))\n",
    "# entity_aliases = load_aliases('wikidata5m_entity.txt')\n",
    "# relation_aliases = load_aliases('wikidata5m_relation.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_files = {k:[] for k in model2dir.keys()}\n",
    "# results = {k:[] for k in model2dir.keys()}\n",
    "# results_data = {k:[] for k in model2dir.keys()}\n",
    "# all_entities = []\n",
    "# results_ids = {k:{} for k in model2dir.keys()}\n",
    "# question_answers_models = {}\n",
    "# model2ks = {}\n",
    "# NUM_SAMPLES = 250\n",
    "# for model, dir in model2dir.items():\n",
    "#     ks = set()\n",
    "#     for file in os.listdir(dir):\n",
    "#         try:\n",
    "#             experiment_results = pickle.load(open(os.path.join(dir, file), 'rb'))\n",
    "#         except:\n",
    "#             print(os.path.join(dir, file))\n",
    "#             continue\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         idx = file.index('Q')\n",
    "#         all_entities.append(file[idx:-4])\n",
    "#         model_files[model].append(file[idx:-4])\n",
    "#         unique_queries = {}\n",
    "#         repeat_queries = 0\n",
    "#         unique_paths = set()\n",
    "#         if len(experiment_results) < 10:\n",
    "#             experiment_results = experiment_results[0]\n",
    "#         for i, result in enumerate(experiment_results):\n",
    "#             if i >= NUM_SAMPLES:\n",
    "#                 break\n",
    "#             if type(result) == str:\n",
    "#                 print(result)\n",
    "#                 print(os.path.join(dir, file))\n",
    "#                 raise ValueError('Error in file: ', file)\n",
    "#             if result['result'][0] == 1:\n",
    "#                 correct += 1\n",
    "#             if result['question'] not in unique_queries:\n",
    "#                 unique_queries[result['question']] = []\n",
    "#             path = tuple(result['path_id'])\n",
    "#             ks.add(len(path)-1)\n",
    "#             unique_paths.add(path)\n",
    "#             if path not in question_answers_models:\n",
    "#                 question_answers_models[path]= {}\n",
    "            \n",
    "#             if model not in question_answers_models[path]:\n",
    "#                 question_answers_models[path][model] = []\n",
    "#             if 'distractor' in result:\n",
    "#                 question_answers_models[path][model].append({'query': result['question'], 'eval': result['result'][0], \n",
    "#                                                          'context': result['context'], 'model_answer':result['model_answer'], \n",
    "#                                                          'correct_answers':result['correct_answers'], 'correct_ids':result['correct_ids'],\n",
    "#                                                          'answer_options':result['options'], 'correct_ans_num':result['correct_ans_num'], \n",
    "#                                                          'distractor':result['distractor']})\n",
    "#             else:\n",
    "#                 question_answers_models[path][model].append({'query': result['question'], 'eval': result['result'][0], \n",
    "#                                                          'context': result['context'], 'model_answer':result['model_answer'], \n",
    "#                                                          'correct_answers':result['correct_answers'], 'correct_ids':result['correct_ids'],\n",
    "#                                                          'answer_options':result['options'], 'correct_ans_num':result['correct_ans_num']})\n",
    "#             unique_queries[result['question']].append(result['result'][0])\n",
    "#             total += 1\n",
    "#         if total > NUM_SAMPLES:\n",
    "#             raise ValueError('Error in file total: ', file, total)\n",
    "#         results_ids[model][file[idx:-4]] = (correct, total, correct/total)\n",
    "#         results[model].append(proportion_confint(correct, total, method='beta'))\n",
    "#         repeat_queries = total - len(unique_queries)\n",
    "#         same_query_accuracy = 0\n",
    "#         count_same = 0\n",
    "#         for query in unique_queries:\n",
    "#             if len(unique_queries[query]) > 1:\n",
    "#                 if np.mean(unique_queries[query]) > 0:\n",
    "#                     same_query_accuracy += np.mean(unique_queries[query])\n",
    "#                     count_same += 1\n",
    "#         if count_same == 0:\n",
    "#             same_query_accuracy = 1.0\n",
    "#         else:\n",
    "#             same_query_accuracy = same_query_accuracy/count_same\n",
    "#         results_data[model].append((correct, total, len(unique_queries), repeat_queries, same_query_accuracy))\n",
    "#     model2ks[model] = ks\n",
    "# print(\"total number of subgraphs: \", {key:len(value)for key, value in results.items()}, \n",
    "#       \"\\nmean lower bounds per subgraph: \", {key:(np.mean([v[0] for v in value]), np.std([v[0] for v in value])) for key, value in results.items()}, \n",
    "#       \"\\nmean upper bounds per subgraph: \", {key:(np.mean([v[1] for v in value]), np.std([v[1] for v in value])) for key, value in results.items()},\n",
    "#       '\\nmean total questions per subgraph: ', {key:(np.mean([v[1] for v in value]), np.std([v[1] for v in value])) for key, value in results_data.items()}, \n",
    "#       \"\\nmean correct answers per subgraph: \", {key:(np.mean([v[0] for v in results_data[key]]), np.std([v[1] for v in results_data[key]])) for key in results_data.keys()},\n",
    "#       \"\\nmean accuracy per subgraph: \", {key:(np.mean([v[0]/v[1] for v in value]), np.std([v[0]/v[1] for v in value])) for key, value in results_data.items()},\n",
    "#       \"\\nmean unique queries per subgraph: \", {key:(np.mean([v[2] for v in value]), np.std([v[2] for v in value])) for key, value in results_data.items()},\n",
    "#       \"\\nmean repeat queries per subgraph: \", {key:(np.mean([v[3] for v in value]), np.std([v[3] for v in value])) for key, value in results_data.items()},\n",
    "#       \"\\nmean same query accuracy per subgraph: \", {key:(np.mean([v[4] for v in value]), np.std([v[4] for v in value])) for key, value in results_data.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get answer samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import random\n",
    "\n",
    "# # Assuming 'question_answers_models' is defined and contains your data\n",
    "# rows = []\n",
    "# model_columns = []\n",
    "# for path_id, models in question_answers_models.items():\n",
    "#     # Iterate over each model and their entries\n",
    "#     for model_id, entries in models.items():\n",
    "#         # Process each entry for the current model\n",
    "#         entry = random.choice(entries)\n",
    "#         path_names = [id2name[x] for x in path_id]\n",
    "#         row = {'path_id': path_id, 'path_names':path_names}\n",
    "#         # Include query, context, and correct answers\n",
    "#         sampled_values = {'query': entry['query'], 'correct_answers': entry['correct_answers'], 'answer_options':entry['answer_options'], \n",
    "#                           'correct_ans_num':entry['correct_ans_num'], 'context': entry['context']}\n",
    "#         row.update(sampled_values)\n",
    "\n",
    "#         # Include model's answer and its evaluation\n",
    "#         model_answer_col = 'model_answer'\n",
    "#         model_eval_col = 'model_eval'\n",
    "#         row['model_id'] = model_id\n",
    "#         row['correct_ids'] = entry['correct_ids']\n",
    "#         row[model_answer_col] = entry['model_answer']\n",
    "#         row[model_eval_col] = entry['eval']\n",
    "#         row['answer_options'] = [id2name[opt] for opt in entry['answer_options']]\n",
    "#         row['correct_ans_num'] = entry['correct_ans_num']\n",
    "#         row['context'] = entry['context']\n",
    "#         if 'distractor' in entry and entry['distractor'] is not None:\n",
    "#             row['distractor'] = id2name[entry['distractor']]\n",
    "#         else:\n",
    "#             row['distractor'] = None\n",
    "\n",
    "#         if model_answer_col not in model_columns:\n",
    "#             model_columns.append(model_answer_col)\n",
    "#         if model_eval_col not in model_columns:\n",
    "#             model_columns.append(model_eval_col)\n",
    "#         if 'model_id' not in model_columns:\n",
    "#             model_columns.append('model_id')\n",
    "#         if 'correct_ids' not in model_columns:\n",
    "#             model_columns.append('correct_ids')\n",
    "#         if 'answer_options' not in model_columns:\n",
    "#             model_columns.append('answer_options')\n",
    "#         if 'correct_ans_num' not in model_columns:\n",
    "#             model_columns.append('correct_ans_num')\n",
    "#         if 'distractor' not in model_columns:\n",
    "#             model_columns.append('distractor')\n",
    "#         if 'context' not in model_columns:\n",
    "#             model_columns.append('context')\n",
    "\n",
    "#         # Add the filled row to the rows list\n",
    "#         rows.append(row)\n",
    "\n",
    "# # Creating the DataFrame\n",
    "# df = pd.DataFrame(rows)\n",
    "# static_columns = ['path_id', 'query', 'correct_answers', 'path_names']\n",
    "# ordered_columns = static_columns + sorted(model_columns)  # Sort or maintain order of model columns as needed\n",
    "# df = df[ordered_columns]\n",
    "# print(df.shape)\n",
    "\n",
    "# # Assuming 'df' is your DataFrame\n",
    "\n",
    "# # Calculate the maximum index to ensure we don't go out of bounds\n",
    "# max_index = len(df) - (len(df) % 121)\n",
    "\n",
    "# # Generate a list of start indices\n",
    "# start_indices = np.arange(0, max_index, 121)\n",
    "\n",
    "# # List to hold the groups\n",
    "# grouped_rows = []\n",
    "\n",
    "# # Loop through each start index and get the consecutive 5 rows\n",
    "# for start in start_indices:\n",
    "#     if random.random() < 0.6:\n",
    "#         continue\n",
    "#     group = df.iloc[start:start +11]  # Select the rows from 'start' to 'start+4'\n",
    "#     grouped_rows.append(group)\n",
    "\n",
    "# # Concatenate all the groups into a new DataFrame\n",
    "# sampled_df = pd.concat(grouped_rows)\n",
    "\n",
    "# print(sampled_df.shape)\n",
    "# # Print the resulting DataFrame\n",
    "# sampled_df.head(15)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
