{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyBV15TCnxUwZTzU0qPrypPO-f3OzREpRbs'\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import ChatModel, InputOutputTextPair\n",
    "from vertexai.preview.generative_models import GenerativeModel, ChatSession\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiRAGOneStore(documents_dir='chandler_documents', openai=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.utilities import SerpAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "question_answerer = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_name, \n",
    "    tokenizer=tokenizer,\n",
    "    device=1\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cuda'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=40)\n",
    "doc_dbs = []\n",
    "files = []\n",
    "for file in ['Bhagwanth_Khuba.txt', 'Karnataka.txt', 'Government_of_Karnataka.txt']:\n",
    "    doc_path = '/home/vvjain3/rag-llm-verify/india_documents/' + file\n",
    "    loader = TextLoader(doc_path)\n",
    "    doc = loader.load()\n",
    "    texts = text_splitter.split_documents(doc)\n",
    "    collection_name = file[:-4]\n",
    "    if file == 'Baden-WÃ¼rttemberg.txt':\n",
    "        collection_name = 'Baden-Wurttemberg'\n",
    "    collection_name = collection_name.replace(',', '_')\n",
    "    doc_db = Chroma.from_documents(texts, embeddings, collection_name=collection_name)\n",
    "    doc_dbs.append(doc_db)\n",
    "    files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhagwant = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=doc_dbs[0].as_retriever())\n",
    "karnataka = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=doc_dbs[1].as_retriever())\n",
    "state_karna = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=doc_dbs[2].as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=doc_dbs[0].as_retriever())\n",
    "harvard = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=doc_dbs[1].as_retriever())\n",
    "cambridge = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=doc_dbs[2].as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Bhagwant Khuba\",\n",
    "        func=bhagwant.run,\n",
    "        description=\"useful for when you need to answer questions about Bhagwant Khuba\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"karnataka\",\n",
    "        func=karnataka.run,\n",
    "        description=\"useful for when you need to answer questions about Karnataka\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Government of Karnataka\",\n",
    "        func=state_karna.run,\n",
    "        description=\"useful for when you need to answer questions about Government of Karnataka\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Mark Zuckerberg\",\n",
    "        func=mark.run,\n",
    "        description=\"useful for when you need to answer questions about Mark Zuckerberg and Facebook\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Harvard University\",\n",
    "        func=harvard.run,\n",
    "        description=\"useful for when you need to answer questions about Harvard University and Cambridge\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Cambridge, Massachusetts\",\n",
    "        func=cambridge.run,\n",
    "        description=\"useful for when you need to answer questions about Cambridge, Massachusetts and Harvard University\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\n",
    "    \"What is the the birth place of Bhagwanth_Khuba?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer.__call__(\"Who is the deputy of the president of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRAG():\n",
    "    def __init__(self, model_name, documents_dir, pipeline_type='text-generation', model_temp=0.7, cuda_device=1) -> None:\n",
    "        self.model_name = model_name\n",
    "        self.documents_dir = documents_dir\n",
    "        self.doc_dbs = []\n",
    "        self.files = []\n",
    "        self.llm = None\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=40)\n",
    "        self.tokenizer = None\n",
    "        self.pipeline_type = pipeline_type\n",
    "        self.model_temp = model_temp\n",
    "        self.retrieval_qas = []\n",
    "        self.tools = []\n",
    "        self.agent = None\n",
    "        self.cuda_device = cuda_device\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cuda'})\n",
    "        self.initialize_llm()\n",
    "        self.initialize_retriever()\n",
    "    def initialize_llm(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, padding=True, truncation=True, max_length=512)\n",
    "        # Define a question-answering pipeline using the model and tokenizer\n",
    "        question_answerer = pipeline(\n",
    "            self.pipeline_type, \n",
    "            model=self.model_name, \n",
    "            tokenizer=self.tokenizer,\n",
    "            device=self.cuda_device\n",
    "        )\n",
    "\n",
    "        # Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "        # with additional model-specific arguments (temperature and max_length)\n",
    "        self.llm = HuggingFacePipeline(\n",
    "            pipeline=question_answerer,\n",
    "            model_kwargs={\"temperature\": self.model_temp, \"max_length\": 512},\n",
    "        )\n",
    "    def initialize_retriever(self):\n",
    "        self.tools = []\n",
    "        self.retrieval_qas = []\n",
    "        self.doc_dbs = []\n",
    "        self.files = []\n",
    "        print('initializing retriever')\n",
    "        for file in os.listdir(self.documents_dir):\n",
    "            doc_path = os.path.join(self.documents_dir, file)\n",
    "            loader = TextLoader(doc_path)\n",
    "            doc = loader.load()\n",
    "            texts = self.text_splitter.split_documents(doc)\n",
    "            collection_name = file[:-4]\n",
    "            collection_name = collection_name.replace(',', '_')\n",
    "            collection_name = collection_name.replace('.', '_')\n",
    "            collection_name = collection_name.replace('(', '')\n",
    "            collection_name = collection_name.replace(')', '')\n",
    "            if len(collection_name) > 60:\n",
    "                collection_name = collection_name[:60]\n",
    "            doc_db = Chroma.from_documents(texts, self.embeddings, collection_name=collection_name)\n",
    "            self.doc_dbs.append(doc_db)\n",
    "            self.files.append(file[:-4])\n",
    "        \n",
    "        for i in range(len(self.doc_dbs)):\n",
    "            self.retrieval_qas.append(RetrievalQA.from_chain_type(llm=self.llm, chain_type='stuff', retriever=self.doc_dbs[i].as_retriever()))\n",
    "            self.files[i] = self.files[i].replace('_', ' ')\n",
    "            self.tools.append(Tool(\n",
    "                    name=self.files[i],\n",
    "                    func=self.retrieval_qas[i].run,\n",
    "                    description=f\"useful for when you need to answer questions about {self.files[i]}\",\n",
    "                ))\n",
    "        # self.tools = []\n",
    "        self.agent = initialize_agent(self.tools, self.llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "    \n",
    "    def run(self, query):\n",
    "        return self.agent.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_rag = MultiRAG('lmsys/vicuna-7b-v1.5', '/home/vvjain3/rag-llm-verify/info/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_rag.documents_dir = '/home/vvjain3/rag-llm-verify/info/'\n",
    "multi_rag.initialize_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_rag.run(\"Where is Harvard University?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRAGAll():\n",
    "    def __init__(self, model_name, documents_dir, pipeline_type='text-generation', model_temp=0.7, cuda_device=1) -> None:\n",
    "        self.model_name = model_name\n",
    "        self.documents_dir = documents_dir\n",
    "        self.doc_db = None\n",
    "        self.llm = None\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=40)\n",
    "        self.tokenizer = None\n",
    "        self.pipeline_type = pipeline_type\n",
    "        self.model_temp = model_temp\n",
    "        self.tools = None\n",
    "        self.agent = None\n",
    "        self.cuda_device = cuda_device\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cuda'})\n",
    "        self.retrieval_qa = None\n",
    "        self.initialize_llm()\n",
    "        self.initialize_retriever()\n",
    "    def initialize_llm(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, padding=True, truncation=True, max_length=512)\n",
    "        # Define a question-answering pipeline using the model and tokenizer\n",
    "        question_answerer = pipeline(\n",
    "            self.pipeline_type, \n",
    "            model=self.model_name, \n",
    "            tokenizer=self.tokenizer,\n",
    "            device=self.cuda_device\n",
    "        )\n",
    "\n",
    "        # Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "        # with additional model-specific arguments (temperature and max_length)\n",
    "        self.llm = HuggingFacePipeline(\n",
    "            pipeline=question_answerer,\n",
    "            model_kwargs={\"temperature\": self.model_temp, \"max_length\": 512},\n",
    "        )\n",
    "    def initialize_retriever(self):\n",
    "        del self.doc_db\n",
    "        del self.retrieval_qa\n",
    "        del self.tools\n",
    "        del self.agent\n",
    "        self.tools = None\n",
    "        self.retrieval_qa = None\n",
    "        self.doc_db = None\n",
    "        print('initializing retriever')\n",
    "        all_texts = []\n",
    "        for file in os.listdir(self.documents_dir):\n",
    "            doc_path = os.path.join(self.documents_dir, file)\n",
    "            loader = TextLoader(doc_path)\n",
    "            doc = loader.load()\n",
    "            texts = self.text_splitter.split_documents(doc)\n",
    "            all_texts.extend(texts)\n",
    "            # collection_name = file[:-4]\n",
    "            # collection_name = collection_name.replace(',', '_')\n",
    "            # collection_name = collection_name.replace('.', '_')\n",
    "            # collection_name = collection_name.replace('(', '')\n",
    "            # collection_name = collection_name.replace(')', '')\n",
    "            # if len(collection_name) > 60:\n",
    "            #     collection_name = collection_name[:60]\n",
    "            # doc_db = Chroma.from_documents(texts, self.embeddings, collection_name=collection_name)\n",
    "            # self.doc_dbs.append(doc_db)\n",
    "            # self.files.append(file[:-4])\n",
    "        self.doc_db = Chroma.from_documents(all_texts, self.embeddings, collection_name='all')\n",
    "        \n",
    "        # for i in range(len(self.doc_dbs)):\n",
    "        self.retrieval_qa = RetrievalQA.from_chain_type(llm=self.llm, chain_type='stuff', retriever=self.doc_db.as_retriever())\n",
    "        #     self.files[i] = self.files[i].replace('_', ' ')\n",
    "        self.tools = [Tool(\n",
    "                name=\"Intermediate Answer\",\n",
    "                func=self.retrieval_qa.run,\n",
    "                description=f\"useful for when you need to answer questions about anything\",\n",
    "            )]\n",
    "        # self.tools = []\n",
    "        self.agent = initialize_agent(self.tools, self.llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True, handle_parsing_errors=True)\n",
    "    \n",
    "    def run(self, query):\n",
    "        return self.agent.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_rag_all = MultiRAGAll('lmsys/vicuna-7b-v1.3', '/home/vvjain3/rag-llm-verify/sample_docs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_rag_all.documents_dir = '/home/vvjain3/rag-llm-verify/sample_docs/'\n",
    "multi_rag_all.initialize_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = multi_rag_all.run(\"Who is Bhagwant Khuba?\")\n",
    "except Exception as e:\n",
    "    response = str(e)\n",
    "    if not response.startswith(\"Could not parse`\"):\n",
    "        raise e\n",
    "    response = response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_rag_all.llm.pipeline(\"Who is Bhagwant Khuba?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
