{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 16:17:28.564258: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 16:17:28.608017: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-16 16:17:28.608044: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-16 16:17:28.609128: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-16 16:17:28.616169: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 16:17:29.633365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import argparse\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import gc\n",
    "from unidecode import unidecode\n",
    "import pickle\n",
    "import torch\n",
    "import gc\n",
    "import gc\n",
    "import argparse\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "from fastchat.model import load_model, get_conversation_template, add_model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5b14568f0748158be7f0522b2de023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "gpu_map = GPU_MAP = {0: \"30GiB\", 1: \"15GiB\", 2: \"0GiB\", 3: \"0GiB\", \"cpu\":\"120GiB\"}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', max_memory=gpu_map, load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "# bnb_4bit_compute_dtype=torch.float16)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', max_memory=gpu_map, torch_dtype=torch.float16, token='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')\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', max_memory=gpu_map, torch_dtype=torch.bfloat16, load_in_8bit=True)\n",
    "if '<pad>' not in tokenizer.get_vocab():\n",
    "    # Add the pad token\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32000, 32000, 32000, 32000, 32000, 32000,     1,   733, 16289, 28793,\n",
       "          6526,   349,   272,  4951,   302,  7035, 11030,  3574,  1863,   297,\n",
       "           574,  2899, 28723,   733, 28748, 16289, 28793],\n",
       "        [    1,   733, 16289, 28793,  1824,   349,   272, 12465, 28733,  1779,\n",
       "           331,  1129,   302,  7035, 11030,  1739,  1863,   304, 13614,   501,\n",
       "           297,   574,  2899,   733, 28748, 16289, 28793]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = []\n",
    "tokenizer.padding_side = 'right'\n",
    "prompts.append([{\"role\": \"user\", \"content\": f\"\"\"Who is the president of USA?.Be direct in your response.\"\"\"}])\n",
    "prompts.append([{\"role\": \"user\", \"content\": f\"\"\"What is the vice-preseident of USA?. Be direct and succint in your response\"\"\"}])\n",
    "input_ids = tokenizer.apply_chat_template(prompts, return_tensors=\"pt\", padding=True)\n",
    "# input_ids = tokenizer.apply_chat_template(prompts, return_tensors=\"pt\", tokenize=False)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm an AI and don't have the ability to be direct in the sense of having up-to-date knowledge or access to real-time information. However, I can tell you that as of my last update, the President of the United States is Joe Biden. He assumed office on January 20, 2021. Please check a reliable source for any potential\", 'As of my current knowledge up to 2021, the Vice President of the United States is Kamala Harris.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = input_ids.to('cuda:0')\n",
    "generated_ids= model.generate(input_ids, max_new_tokens=80, pad_token_id=tokenizer.eos_token_id, do_sample=True, temperature=1.0)\n",
    "responses = tokenizer.batch_decode(generated_ids[:, input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
